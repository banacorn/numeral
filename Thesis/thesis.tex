
\title{Generalizing Positional Numeral Systems}
\author{
    Luā Tîng-Giān
}
\date{\today}

\documentclass[12pt, a4paper]{article}
% \linespread{1.5}

% for unicode symbols
\usepackage{fontspec}
\setmonofont{DejaVu Sans Mono}

% for maths symbols
\usepackage{mathtools}
\usepackage{bm}


% for CJK
\usepackage{xeCJK}

\usepackage{color}
\definecolor{light-gray}{gray}{0.95}

\usepackage{listings}
\lstset{ %
  basicstyle=\ttfamily\footnotesize,              % the size of the fonts that are used for the code
  backgroundcolor=\color{light-gray},
  xleftmargin=0.5cm,
  frame=tlbr,
  framesep=10pt,
  framerule=0pt
}

\begin{document}
\maketitle

\begin{abstract}
% Abstact [draft]
Numbers are everywhere in our daily lives, and positional numeral systems are
arguably the most important and common representation of numbers. In this work
we have constructed a generalized positional numeral system in Agda to model many
of these representations, and investigate some of their properties and relationship
with the classical unary representation of the natural numbers.

% TODO: maybe say more about that predicate universe stuff
\end{abstract}

\section{Introduction}\label{introduction}

\subsection{Positional numeral systems}
% [reviewed: 1]

A numeral system is a writing system for expressing numbers, and humans have
invented various kinds of numeral systems throughout history. Most of the
systems we are using today are positional notations\cite{knuth1998art} because
they can express infinite numbers with just a finite set of symbols called
\textbf{digits}.

Positional numeral systems represent a number by adding up a sequence of digits
of different orders of magnitude. Take a decimal number for example:

$$ (2016)_{10} = 2\times10^3 + 0\times10^2 + 1\times10^1 + 6\times10^0 $$

$ 6 $ is called \textit{least significant digit} and $ 2 $ is called the
\textit{most significant digit} in this example. From now on, except when writing
decimal numbers, we will write down numbers in reverse order, from the least
significant digit to the most significant digit, like $ 6102 $.

To make things clear, we call a sequence of digits a \textbf{numeral}, or a \textbf{notation};
and the number it expresses a \textbf{value}, or simply a \textbf{number}. That is,
we distinguish syntax from semantics. Syntax bears no meaning; its semantics can
only be carried out by converting to some other syntax. We call a function that
converts notations to values an \textbf{evaluator}, and the process an
\textbf{evaluation}.

\subsubsection{Symtems of different bases}

These numeral systems can take on different \textit{bases}. The ubiquitous decimal
numeral system as we know has the base of 10. While the binaries that can be found
in our machines nowadays has the base of 2. To evaluate a notation of certain
system of base:

$$
    ({d_0d_1d_2d_3...})_{base}
    =
    d_0\times base^0 + d_1\times base^1 + d_2\times base^2 + d_3\times base^3 ...
$$

Where $ d_{n} $ is a digit that ranges from $ 0 $ to $ base - 1 $ for all $ n $.


\subsubsection{Digits of different ranges}

Some computer scientists and mathematicians seem to be more comfortable with
unary (base-1) numbers because they are isomorphic to the natural numbers à la Peano.

$$
    (1111)_{1} \cong
        \overbrace{\text{suc (suc (suc (suc}}^4 + \text{ zero)))}
$$

Statements established on such construction can be proven using mathematical
induction. Moreover, people have implemented and proven a great deal of functions
and properties on these unary numbers because they are easy to work with.

However, the formula we have just put down for evaluating positional numeral systems
doesn't work for unary numbers, as it requires the digits to range from $ 0 $ to
$ base - 1 $. That is, the only digit has to be $ 0 $, yet the only digit unary
numbers have is $ 1 $.

To cooperate unary numbers, we relax the constraint on the range of digits
by introducing a new variable, \textit{offset}:

$$
    ({d_0d_1d_2d_3...})_{base}
    =
    d_0\times base^0 + d_1\times base^1 + d_2\times base^2 + d_3\times base^3 ...
$$

Where $ d_{n} $ ranges from \textit{offset} to \textit{offset + base - 1} for
all $ n $. Now that unary numbers would have an offset of $ 1 $,
and systems of other bases would have offsets of $ 0 $.

\subsubsection{Redundancy}

With the generalization of \textit{base} and \textit{offset}, so far we have been
able to cover some different kinds of numeral systems, but the binary numeral
system that is implemented in virtually all arithmetic logic unit (ALU) hardware
is not among them.

In our representation, operations such as addition would take $ O(log{}n)$ for some
number $ n $ in a system where $ base \text{\textgreater} 1 $. Since the number $ n $
would have length $ log_{base} n $ and operations on each digit should be constant.
However, these operations only takes constant time in machines!

That seems to be a big performance issue, but there's a catch! Because our
representation is capable of what is called \textit{arbitrary-precision arithmetic},
i.e., it could perform calculations on numbers of arbitrary size while the binary
numbers that reside in machines are bounded by the hardware, which could only
perform \textit{fixed-precision arithmetic}.

Surprisingly, we could fit these binary numbers into our representation with
just a tweak. If we allow a system to have more digits, then a fixed-precision
binary number can be regarded as a single digit! To illustrate this, a 32-bit
binary number would become a single digit that ranges from $ 0 $ to $ 2^{32} $,
while everything else including the base remains the same.

Formerly in our representation, there are exactly \textit{base} number of digits
that range from:

$$
    \text{offset}  ...  \text{offset} + \text{base} - 1
$$

We introduce a new index \textit{\#digit} to generalize the number of digits.
Now they range from:

$$
    \text{offset}  ...  \text{offset} + \text{\#digit} - 1
$$

Here's a table of the configurations about the systems that we've addressed:

\begin{center}
    \begin{tabular}{l*{3}{r}}
    Numeral system      & base  & \#digit    & offset    \\
    \hline
    Decimal             & 10    & 10        & 0         \\
    Binary              & 2     & 2         & 0         \\
    Unary               & 1     & 1         & 1         \\
    Int32               & 2     & $ 2^{32} $ & 0        \\
    \end{tabular}
\end{center}

Consider this numeral system, the oridinary binary numbers with an extra digit:
$ 2 $.

\begin{center}
    \begin{tabular}{l*{3}{r}}
    Numeral system      & base  & \#digit    & offset    \\
    \hline
    0-1-2 Binary        & 2     & 3          & 0         \\
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{c*{1}{l}}
    Number (in decimal)  & Notation \\
    \hline
    0       & 0 \\
    1       & 1 \\
    2       & 01, 2 \\
    3       & 11 \\
    4       & 001, 21 \\
    5       & 101, 12 \\
    \end{tabular}
\end{center}

Such a numeral system is said to be \textbf{redundant}, because there are more than one
way to represent a number. In fact, systems that allow $ 0 $ as one of the digits
must be redundant, since we can always take a number and add leading zeros without
changing it's value. Systems that does not have zeros are said to be \textbf{zeroless}.

We will see that there's a deep connection between data
structures and  numeral systems. Data structures modeled after redundant numeral
systems have some interesting properties.

\subsubsection{Numerical representation}

One may notice that the structure of unary numbers looks suspiciously similar
to that of lists'. Let's compare their definition in Haskell.

% use minipage for juxtaposing two blocks of codes
\noindent\begin{minipage}{.45\textwidth}
\begin{lstlisting}
data Nat = Zero
         | Suc Nat
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.48\textwidth}
\begin{lstlisting}
data List a = Nil
            | Cons a (List a)
\end{lstlisting}
\end{minipage}

If we replace every {\lstinline|Cons _|} with {\lstinline|Suc|} and {\lstinline|Nil|}
with {\lstinline|Zero|}, then a list becomes an unary number.
And that is exactly what the {\lstinline|length|} function,
a homomorphism from lists to unary numbers, does.

Now let's compare addition on unary number and merge (append) on lists:

\noindent\begin{minipage}{.48\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
add : Nat → Nat → Nat
add Zero    y = y
add (Suc x) y =
    Suc (add x y)
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize]
append : List a → List a → List a
append Nil         ys = ys
append (Cons x xs) ys =
    Cons x (append xs ys)
\end{lstlisting}
\end{minipage}

Aside from having virtually identical implementations, operations on unary numbers
and lists both have the same time complexity. Incrementing a unary number takes
$ O(1) $, inserting an element into a list also takes $ O(1) $; adding two unary
numbers takes $ O(n) $, appending a list to another also takes $ O(n) $.

If we look at implementations and operations of binary numbers and binomial
heaps, the resemblances are also uncanny.

[insert some images here]

The strong analogy between positional numeral systems and certain data structures
suggests that, numeral systems can serve as templates for designing containers.
Such data structures are called \textbf{Numerical Representations}\cite{okasaki1996purely}
\cite{hinze1998numerical}.

[say something about redundant data structures]

% \subsubsection{From numeral systems to numbers}

% But that simplicity comes at a cost, the time complexity of functions defined on
% unary numbers is significantly

% People have implemented and proven a great deal of functions and properties on
% these unary numbers because they are easy to work with. But the simplicity comes
% at a cost, the time complexity of functions defined on unary numbers is significantly
% higher than those defined on systems of other bases, say, binary numbers. Hence,
% it's not practical in doing heavy calculations.

% The \textit{numbers} we are constructing in this work are the \textit{natural numbers}.
% We learned how to count with these numbers when we are little. But what are numbers, really?

% \paragraph{Recursive}

% Paul Benacerraf once argued\cite{benacerraf1965numbers} that, there are two kinds
% of \textit{counting} which corresponds to \textbf{transitive} and \textbf{intransitive}
% uses of the verb "to count". Transitive counting, in his sense, is to assign one
% of the numbers to the cardinality of a set, by establish a one-to-one correspondence
% between the numbers and the objects one is counting, all the way from none to
% all. Intransitive counting, on the other hand, is to generate a sequence of
% notation, that could go as far as we need. And it seems that one can only learn
% how to count intransitively first, before knowning how to count transitively,
% but not vice versa.
%
% He further discussed that the numbers must be at least recursive

% \paragraph{Numbers are recursive}
%
% \paragraph{Internal definition is immaterial}

\paragraph{Outline}
The remainder of the thesis is organized as follows.

% Section~\ref{agda} .

\section{A gental introduction to dependently typed programming in Agda}\label{agda}
% [draft]

There are already plenty of tutorials and introductions of Agda\cite{norell2009dependently}\cite{FLOLAC16DTP}\cite{brutal}.
We will nonetheless compile a simple and self-contained tutorial from the
materials cited above, covering the part (and only the part) we need in this work.

Some of the more advenced constructions (such as views and universes) used in
the following sections will be introduced along the way.

We assume that all readers have some basic understanding of Haskell, and those
who are familiar with Agda and dependently typed programming may skip this chapter.

\subsection{Some basics}
% [draft]

Agda is a \textit{dependently typed functional programming language} and also an
\textit{interactive proof assistant}. It can be both because it's based on
\textit{Martin-Löf type theory}\cite{martin1984intuitionistic}, hence the Curry-Howard
correspondence\cite{sorensen2006lectures}, which states that: "propositions are types"
and "proofs are programs". In other words, proving theorems and writing programs
are essentially the same. In Agda we are free to interchange between these two
interpretations. The current version (Agda2) is a completely rewrite by Ulf Norell
during his PhD at Chalmers University of Technology.

We say that Agda is interactive because proving
theorems involves a lot of conversations between the programmer and the type checker.
And it is often difficult, if not impossible, to develop and prove a theorom at one stroke.
Just like programming, the process is incremental.
So Agda allows us to leave some "holes" in a program, refine them gradually, and
complete the proofs "hole by hole".

Take this half-finished function definition for instance, we could leave out the
right-hand side.

\begin{lstlisting}
is-zero : Int → Bool
is-zero x = ?
\end{lstlisting}

In practice, we would ask, for example, "what's the type of the goal?",
 "what's the context of this case?", etc. And Agda would reply us with:

\begin{lstlisting}
GOAL : Bool
x : Int
\end{lstlisting}

Then we may ask Agda to pattern match on {\lstinline|x|} and rewrite the program for us:

\begin{lstlisting}
is-zero : Int → Bool
is-zero zero    = ?
is-zero (suc x) = ?
\end{lstlisting}

We could fulfill these goals by giving an answer, we may even ask Agda to solve
the problem for us, if it is not too difficult.

\begin{lstlisting}
is-zero : Int → Bool
is-zero zero    = true
is-zero (suc x) = false
\end{lstlisting}

After all of the goals have been accomplished and type-checked, we consider the
program to be finished. Often, there's not much point in running a Agda program,
because it's mostly about static constructions that is checked in compile-time.
This is basically what pragramming and proving things looks like in Agda.

\subsection{Simply typed programming in Agda}

Since Agda was heavily influenced by Haskell, simply typed programming in Agda
is similar to that in Haskell.

\paragraph{Datatypes}
% [draft]

Unlike in other programming languages, there are no "built-in"
datatypes such as \textit{Int}, \textit{String}, or \textit{Bool}.
The reason is that they can all be created out of thin air, so why bother?

Datatypes are introduced with {\lstinline|data|} declarations.
Here is a classical example, the type of booleans.

\begin{lstlisting}
data Bool : Set where
    true  : Bool
    false : Bool
\end{lstlisting}

The name of the datatype ({\lstinline|Bool|}) and its constructors
({\lstinline|true|} and {\lstinline|false|}) are brought into scope in this declaration.
This notation also allow us to explicitly specify the types of these newly introduced entities.

\begin{enumerate}
    \item {\lstinline|Bool|} has the type of {\lstinline|Set|}\footnote{{\lstinline|Set|} is the type of small types, and {\lstinline|Set₁|} is the type
of {\lstinline|Set|}, and so on. They form a hierarchy of types.}
    \item {\lstinline|true|} has the type of {\lstinline|Bool|}
    \item {\lstinline|false|} has the type of {\lstinline|Bool|}
\end{enumerate}

\paragraph{Pattern matching}
% [draft]
Similar to Haskell, datatypes are eliminated with pattern matching.

Here's a function that pattern matches on {\lstinline|Bool|}.

\begin{lstlisting}
not : Bool → Bool
not true  = false
not false = true
\end{lstlisting}

Agda is a \textit{total} language, that means partial functions are not valid constructions.
Functions are guarantee to terminate and will not crash on all possible inputs.
The following example won't be accecpted by the type checker, because the case
{\lstinline|false|} is missing.

\begin{lstlisting}
not : Bool → Bool
not true  = false
\end{lstlisting}

In practice, Agda would automatically expand all of the cases for us on demand.

\paragraph{Inductive datatype} Let's move on to a more interesting datatype with
inductive definition. Here's the type of natural numbers.
% [draft]
\begin{lstlisting}
data ℕ : Set where
    zero : ℕ
    suc : ℕ → ℕ
\end{lstlisting}

The decimal number "4" is represented as {\lstinline|suc (suc (suc (suc zero)))|}.
Agda also accepts arabic literals if the datatype {\lstinline|ℕ|} complies with
certain language pragma.

Addition on {\lstinline|ℕ|} can be defined as a recursive function.

\begin{lstlisting}
_+_ : ℕ → ℕ → ℕ
zero  + y = y
suc x + y = suc (x + y)
\end{lstlisting}

We define {\lstinline|_+_|} by pattern matching on the first argument, which results
in two cases: the base case, and the inductive step. We are allowed to make
recursive calls, as long as the type checker is convinced that the function
would terminate.

The underlines surrounding {\lstinline|_+_|} act as placeholders for arguments, making
it an infix function in this instance.

\paragraph{Dependent functions and type arguments}
% [reviewed: 1]
Up till now everything looks much the same as in Haskell, but a problem arises as
we move on to defining something that needs more power of abstraction. Take identity
functions for example:

\begin{lstlisting}
id-Bool : Bool → Bool
id-Bool x = x

id-ℕ : ℕ → ℕ
id-ℕ x = x
\end{lstlisting}

In order to define a more general identity function, those concrete types have
to be abstracted away. That is, we need parametric polymorphism, and this is
where dependent types come into play.

A dependent type is a type whose definition may depend on a value. A dependent
function is a function whose result type may depend on the value of an argument.

In Agda, function types are denoted as:

\begin{lstlisting}
A → B
\end{lstlisting}

Where {\lstinline|A|} is the type of domain and {\lstinline|B|} is the type of
codomain. To let {\lstinline|B|} depends on the value of {\lstinline|A|}, the
value has to \textit{named}, in Agda we write:

\begin{lstlisting}
(x : A) → B x
\end{lstlisting}

The value of {\lstinline|A|} is named {\lstinline|x|} and then fed to {\lstinline|B|}.
As a matter of fact, {\lstinline|A → B|} is just a syntax sugar for {\lstinline|(_ : A) → B|}
with the name of the value being irrelevant. The underline {\lstinline|_|} here
means "I don't bother naming it".

Back to our identity function, if {\lstinline|A|} happens to be {\lstinline|Set|},
the type of all small types, and the result type happens to be solely {\lstinline|x|}:

\begin{lstlisting}
(x : Set) → x
\end{lstlisting}

Voila, we have polymorphism, and thus the identity function can now be defined as:

\begin{lstlisting}
id : (A : Set) → A → A
id A x = x
\end{lstlisting}

{\lstinline|id|} now takes an extra argument, the type of the second arguement.
{\lstinline|id Bool true|} evaluates to {\lstinline|true|}

\paragraph{Implicit arguments}
% [reviewed: 1]

We have implemented an identity function and seen how polymorphism can be modeled
with dependent types. However, the additional argument that the identity function
takes is rather unnecessary, since its value can always be determined by looking
at the type of the second argument.

Fortunately, Agda supports \textit{implicit arguments}, a syntax sugar that could
save us the trouble of having to spell them out. Implicit arguments are enclosed
in curly brackets in the type expression. We are free to dispense with these arguments
when their values are irrelevant to the definition.

\begin{lstlisting}
id : {A : Set} → A → A
id x = x
\end{lstlisting}

Or when the type checker can figure them out on function application.

\begin{lstlisting}
val : Bool
val = id true
\end{lstlisting}

Any arguments can be made implicit, but it does not imply that values of
implicit arguments can always be inferred or derived from context. We can always
make them implicit arguments explicit on application:

\begin{lstlisting}
val : Bool
val = id {Bool} true
\end{lstlisting}

Or when they are relevant to the definition:

\begin{lstlisting}
silly-not : {_ : Bool} → Bool
silly-not {true}  = false
silly-not {false} = true
\end{lstlisting}

\paragraph{More syntax sugars}
% [draft]
We could skip arrows between arguments in parentheses or braces:

\begin{lstlisting}
id : {A : Set} (a : A) → A
id {A} x = x
\end{lstlisting}

And there is a shorthand for merging names of arguments of the same type, implicit or not:

\begin{lstlisting}
const : {A B : Set} → A → B → A
const a _ = a
\end{lstlisting}

Sometimes when the type of some value can be inferred, we could either replace
the type with an underscore, say {\lstinline|(A : _)|}, or we could write it as
{\lstinline|∀ A|}. For the implicit counterpart, {\lstinline|{A : _}|} can be
written as {\lstinline|∀ {A}|}.

\paragraph{Parameterized Datatypes}
% [draft]

Just as functions can be polymorphic, datatypes can be parameterized by other
types, too. The datatype of lists is defined as follows:

\begin{lstlisting}
data List (A : Set) : Set where
    []  : List A
    _∷_ : A → List A → List A
\end{lstlisting}

The scope of the parameters extends over the entire declaration, so they can
appear in the constructors.
Here are the types of the datatype and its constructors.

\begin{lstlisting}
infixr 5 _∷_

[] : {A : Set} → List A
_∷_ : {A : Set} → A → List A → List A
List : Set → Set
\end{lstlisting}

Where {\lstinline|A|} can be anything, even {\lstinline|List (List (List Bool))|},
as long as it is of type {\lstinline|Set|}. {\lstinline|infixr|} specifies the
precedence of the operator {\lstinline|_∷_|}.

\paragraph{Indexed Datatypes}
% [draft]

% Indexed datatypes, or inductive families, allow us to not only

{\lstinline|Vec|} is a datatype that is similar to {\lstinline|List|}, but more
powerful, in that it can tell you not only the type of its element, but also its
length.

\begin{lstlisting}
data Vec (A : Set) : ℕ → Set where
    []  : Vec A zero
    _∷_ : {n : ℕ} → A → Vec A n → Vec A (suc n)
\end{lstlisting}

{\lstinline|Vec A n|} is a vector of values of type {\lstinline|A|} and
has the length of {\lstinline|n|}. Here are some of its inhabitants:

\begin{lstlisting}
nil : Vec Bool zero
nil = []

vec : Vec Bool (suc (suc zero))
vec = true ∷ false ∷ []
\end{lstlisting}

We say that {\lstinline|Vec|} is \textit{parameterized} by a type of {\lstinline|Set|}
and is \textit{indexed} by values of {\lstinline|ℕ|}.
And we distinct indices from parameters. However, it is not obvious how they are
different by looking at the declaration.

Parameters are \textit{parametric}, in the sense that, they have no effect on the "shape" of a datatype.
The choice of parameters only effects which kind of values are placed there.
Pattern matching on parameters does not reveal any insights about their whereabouts.
Because they are \textit{uniform} across all constructors, one can always replace
the value of a parameter with another one of the same type.

On the other hand, indices may affect which inhabitants are allowed in the
datatype. Different constructors may have different indices. In that case, pattern
matching on indices may yield relevant information about their constructors.

For example, if there's term whose type is {\lstinline|Vec Bool zero|}, then
we are certain that the constructor must be {\lstinline|[]|}, and if the type
 is {\lstinline|Vec Bool (suc n)|} for some {\lstinline|n|}, then the constructor
must be {\lstinline|_∷_|}.

We could for instance define a {\lstinline|head|} function that cannot crash.

\begin{lstlisting}
head : ∀ {A n} → Vec A (suc n) → A
head (x ∷ xs) = x
\end{lstlisting}

As a side note, parameters can be thought as a degenerate case of indices whose
distribution of values are uniform across all constructors.

\paragraph{With abstraction}
% [draft]

Say, we want to define {\lstinline|filter|} on {\lstinline|List|}:

\begin{lstlisting}
filter : ∀ {A} → (A → Bool) → List A → List A
filter p [] = []
filter p (x ∷ xs) = ?
\end{lstlisting}

We are stuck here, because the result of {\lstinline|p x|} is only available in
runtime. Fortunately, with abstraction allows us to pattern match on the result
of an intermediate computation by adding the result as an extra argument on the
left-hand side:

\begin{lstlisting}
filter : ∀ {A} → (A → Bool) → List A → List A
filter p [] = []
filter p (x ∷ xs) with f x
filter p (x ∷ xs) | true  = x ∷ filter p xs
filter p (x ∷ xs) | false = filter p xs
\end{lstlisting}

\paragraph{Absurd patterns}
% [draft]

The \textit{unit type}, or \textit{top}, is a datatype inhabited by
exactly one value, denoted {\lstinline|tt|}.

\begin{lstlisting}
data ⊤ : Set where
    tt : ⊤
\end{lstlisting}

The \textit{empty type}, or \textit{bottom}, on the other hand, is a datatype
that is inhabited by nothing at all.

\begin{lstlisting}
data ⊥ : Set where
\end{lstlisting}

These types seem useless, and without constructors, it is impossible to
construct an instance of {\lstinline|⊥|}. What is an type that cannot be
constructed good for?

Say, we want to define a safe {\lstinline|head|} on {\lstinline|List|} that
does not crash on any inputs. Naturally, in a language like Haskell,
we would come up with a predicate like this to filter out empty lists
{\lstinline|[]|} before passing them to {\lstinline|head|}.

\begin{lstlisting}
non-empty : ∀ {A} → List A → Bool
non-empty []       = false
non-empty (x ∷ xs) = true
\end{lstlisting}

The predicate only works at runtime. It is impossible for the type
checker to determine whether the input is empty or not at compile time.

However, things are quite different quite in Agda. With \textit{top} and \textit{bottom},
we could do some tricks on the predicate, making it returns a \textit{Set}, rather
than a \textit{Bool}!

\begin{lstlisting}
non-empty : ∀ {A} → List A → Set
non-empty []       = ⊥
non-empty (x ∷ xs) = ⊤
\end{lstlisting}

Notice that now this predicate is returning a type. So we can use it in the type
expression. {\lstinline|head|} can thus be defined as:

\begin{lstlisting}
head : ∀ {A} → (xs : List A) → non-empty xs → A
head []       proof = ?
head (x ∷ xs) proof = x
\end{lstlisting}

In the {\lstinline|(x ∷ xs)|} case, the arguement {\lstinline|proof|} would
have type {\lstinline|⊤|}, and the right-hand side is simply {\lstinline|x|};
in the {\lstinline|[]|} case, the arguement {\lstinline|proof|} would
have type {\lstinline|⊥|}, but what should be returned at the right-hand side?

It turns out that, the right-hand side of the {\lstinline|[]|} case would be the
least thing to worry about, because it is completely impossible to have such a case.
Recall that {\lstinline|⊥|} has no inhabitants, so if a case has an argument of
that type, it is too good to be true.

Type inhabitance is in general an undecidable problem.
However, when pattern matching on a type that is obviously empty (such as {\lstinline|⊥|}),
Agda allows us to drop the right-hand side and eliminate the arguement with {\lstinline|()|}.

\begin{lstlisting}
head : ∀ {A} → (xs : List A) → non-empty xs → A
head []       ()
head (x ∷ xs) proof = x
\end{lstlisting}

Whenever an empty list is applied to {\lstinline|head|},
the resulting function would have type {\lstinline|head [] : ⊥ → A|},
which is impossible to fulfill unless one could find a value of type {\lstinline|⊥|}.

\paragraph{Propositions as types, proofs as programs}

The previous paragraphs are mostly about the \textit{programming}
aspect of the language, but there is another aspect to it.
Recall the Curry–Howard correspondence, propositions are types and proofs are
programs. A proof exists for a proposition the way that a value inhabits a type.

So {\lstinline|non-empty xs|} is a type, but it can also be thought of as a
proposition stating that {\lstinline|xs|} is not empty.
When {\lstinline|non-empty xs|} evaluates to {\lstinline|⊥|}, no value inhabits
{\lstinline|⊥|}, that means no proof exists for the proposition {\lstinline|⊥|};
when {\lstinline|non-empty xs|} evaluates to {\lstinline|⊥|}, {\lstinline|tt|}
inhabits {\lstinline|⊥|}, a trivial proof exists for the proposition {\lstinline|⊤|}.

In intuitionistic logic, a proposition is considered to be "true" when it is
inhabited by a proof, and considered to be "false" when there exists no proof.
Contrary to classical logic, where propositions evaluates to truth values.
We can see that {\lstinline|⊤|} and {\lstinline|⊥|} correspondes to \textit{true}
and \textit{false} in this sense.

Negation can be defined as a function from a proposition to {\lstinline|⊥|}.

\begin{lstlisting}
¬ : Set → Set
¬ P = P → ⊥
\end{lstlisting}

We could exploit {\lstinline|⊥|} further to deploy the principle of explosion
of intuitionistic logic, which states that: "from falsehood, anything (follows)"
(Latin: \textit{ex falso (sequitur) quodlibet}).

\begin{lstlisting}
⊥-elim : ∀ {Whatever : Set} → ⊥ → Whatever
⊥-elim ()
\end{lstlisting}

\paragraph{Decidable propositions}

A proposition is decidable when it can be proved \textit{or} disapproved.
\footnote{The connective \textit{or} here is not a disjunction in the classical sense.
Either way, a proof or a disproval has to be given.}

\begin{lstlisting}
data Dec (P : Set) : Set where
    yes :   P → Dec P
    no  : ¬ P → Dec P
\end{lstlisting}

{\lstinline|Dec|} is very similar to its two-valued cousin {\lstinline|Bool|},
but way more powerful, because it also explains (with a proof) why a proposition
holds or why it does not.

Suppose we want to know if a natural is even or odd. We know that {\lstinline|zero|}
is an even number, and if a number is even then its successor's successor is also even.

\begin{lstlisting}
    data Even : ℕ → Set where
        base : Even zero
        step : ∀ {n} → Even n → Even (suc (suc n))
\end{lstlisting}

We also need the opposite of {\lstinline|step|} as a lemma.

\begin{lstlisting}
    2-steps-back : ∀ {n} → ¬ (Even n) → ¬ (Even (suc (suc n)))
    2-steps-back ¬p q = ?
\end{lstlisting}

{\lstinline|2-steps-back|} takes two argument instead of one because the return
type {\lstinline|¬ (Even (suc (suc n)))|} is actually a synonym of
{\lstinline|Even (suc (suc n)) → ⊥|}. Pattern matching on the second argument
of type {\lstinline|Even (suc (suc n))|} further reveals that it could only be
constructed by {\lstinline|step|}.  By contradicting {\lstinline|¬p : ¬ (Even n)|}
and {\lstinline|p : Even n|}, we complete the proof of this lemma.

\begin{lstlisting}
    contradiction : ∀ {P Whatever : Set} → P → ¬ P → Whatever
    contradiction p ¬p = ⊥-elim (¬p p)

    two-steps-back : ∀ {n} → ¬ (Even n) → ¬ (Even (suc (suc n)))
    two-steps-back ¬p (step p) = contradiction p ¬p
\end{lstlisting}

Finally, {\lstinline|Even?|} determines a number is even by induction on its
predecessor's predecessor. {\lstinline|step|} and {\lstinline|two-steps-back|}
can be viewed as functions that transforms proofs.

\begin{lstlisting}
    Even? : (n : ℕ) → Dec (Even n)
    Even? zero          = yes base
    Even? (suc zero)    = no (λ ())
    Even? (suc (suc n)) with Even? n
    Even? (suc (suc n)) | yes p = yes (step p)
    Even? (suc (suc n)) | no ¬p = no  (two-steps-back ¬p)
\end{lstlisting}

The syntax of {\lstinline|λ ()|} looks weird, as the result of contracting
an argument of type {\lstinline|⊥|} of a lambda expression {\lstinline|λ x → ?|}.
It is a convention to suffix a decidable function's name with {\lstinline|?|}.

\paragraph{Propositional equality}

Saying that two things are "equal" is a notoriously intricate topic in type theory.
There are many different notions of equality
\cite{equality}. We will not go
into each kind of equalities in depth but only skim through those exist in Agda.

\textit{Definitional equality}, or \textit{intensional equality} is simply a
synonym, a relation between linguistic expressions. It is a primitive judgement
of the system, stating that two things are the same to the type checker
\textbf{by definition}.

\textit{Computational equality} is a slightly more powerful notion.
Two programs are consider equal if they compute (beta-reduce) to the same value.
For example, {\lstinline|1 + 1|} and {\lstinline|2|} are equal in Agda in this notion.

But we cannot say that {\lstinline|a + b|} and {\lstinline|b + a|}
are equal with definitional or computational equality, because this kind of equality
is \textit{extensional}. However, it could be expressed as a \textit{proposition}
with \textit{identity types}.

\begin{lstlisting}
data _≡_ {A : Set} (x : A) : A → Set where
    refl : x ≡ x
\end{lstlisting}

For all {\lstinline|a b : A|}, if {\lstinline|a|} and {\lstinline|b|} are
\textit{computationally equal}, that is, both computes to the same value,
then {\lstinline|refl|} is a proof of {\lstinline|a ≡ b|},
the \textit{propositional equality} of {\lstinline|a|} and {\lstinline|b|}.

{\lstinline|_≡_|} is an equivalence relation. It means that {\lstinline|_≡_|}
is \textit{reflexive} (by definition), \textit{symmetric} and \textit{transitive}.

\begin{lstlisting}
sym : {A : Set} {a b : A} → a ≡ b → b ≡ a
sym refl = refl

trans : {A : Set} {a b c : A} → a ≡ b → b ≡ c → a ≡ c
trans refl refl = refl
\end{lstlisting}

{\lstinline|_≡_|} is congruent, meaning that we could \textbf{substitute equals for equals}.

\begin{lstlisting}
cong : {A B : Set} {a b : A} → (f : A → B) → a ≡ b → f a ≡ f b
cong f refl = refl
\end{lstlisting}

Although these {\lstinline|refl|}s look all the same at term level, they
are proofs of different propositional equalities.

\paragraph{Dotted patterns}

Consider an alternative version of {\lstinline|sym|} on {\lstinline|ℕ|}.

\begin{lstlisting}
sym' : (a b : ℕ) → a ≡ b → b ≡ a
sym' a b eq = ?
\end{lstlisting}

Where {\lstinline|eq|} has type {\lstinline|a ≡ b|}.
If we pattern match on {\lstinline|eq|} then Agda would rewrite {\lstinline|b|}
as {\lstinline|.a|} and the goal type becomes {\lstinline|a ≡ a|}.

\begin{lstlisting}
sym' : (a .a : ℕ) → a ≡ a → a ≡ a
sym' a .a eq = ?
\end{lstlisting}

What happened under the hood is that {\lstinline|a|} and {\lstinline|b|} are
\textit{unified} as the same thing. The second argument is dotted to signify that
it is \textit{constrained} by the first argument {\lstinline|a|}. {\lstinline|a|}
becomes the only argument available for further binding or pattern matching.

\paragraph{Standard library}

It would be inconvenient if we have to construct everything we need from scratch.
Luckily, the community has maintained a standard library that comes with many useful
and common constructions.

The standard library is not "chartered" by the compiler or the type checker,
there's simply nothing special about it. We may as well as roll our own library.
\footnote{Some primitives that require special treatments, such as IO, are take cared with
language pragmas exposed by Agda.}




\section{Proving Properties of Numbers with Equational Reasoning}\label{eq-reasoning}

Because this thesis is mostly about numbers, we will put more focus on
proving numerical properties and equational reasoning.

With propositional equality at our disposal, we will show how to prove properties
such as the commutative property of addition. And as proofs get more complicated,
we will introduce tools for equational reasoning to make life easier.

\paragraph{Right identity of addition}

Recap the definition of addition on {\lstinline|ℕ|}.

\begin{lstlisting}
_+_ : ℕ → ℕ → ℕ
zero  + y = y
suc x + y = suc (x + y)
\end{lstlisting}

{\lstinline|_+_|} is defined by induction on the first argument. That means we
get the \textit{left identity} of addition for free, as {\lstinline|zero + y|}
and {\lstinline|y|} are \textit{computationally equal}. But this is not the case
for the \textit{right identity} of addition. It has to be proven explicitly.

\begin{lstlisting}
+-right-identity : (n : ℕ) → n + 0 ≡ n
+-right-identity zero    = ?0
+-right-identity (suc n) = ?1
\end{lstlisting}

By induction on the only argument, we get two sub-goals:

\begin{lstlisting}
?0 : 0 ≡ 0
?1 : suc (n + 0) ≡ suc n
\end{lstlisting}

{\lstinline|?0|} can be trivially proven with {\lstinline|refl|}.
The type of {\lstinline|?1|} looks a lot like the proposition we are proving,
except that both side of the equation are "coated" in a {\lstinline|suc|}.
With {\lstinline|cong suc : ∀ {x y} → x ≡ y → suc x ≡ suc y|}, we could substitute
something in {\lstinline|suc|} with another if they are also equal, and finish
the proof by recursively calling itself with a \textit{smaller} argument.

\begin{lstlisting}
+-right-identity : ∀ n → n + 0 ≡ n
+-right-identity zero    = refl
+-right-identity (suc n) = cong suc (+-right-identity n)
\end{lstlisting}

\paragraph{Moving suc to the other side}

This is an essential lemma for proving other theorems.
The proof also follows a similar pattern as that of {\lstinline|+-right-identity|}.
\footnote{In fact, all of these proofs (hense programs) can be generalized with
some kinds of \textit{fold}, but that is not the point here.}

\begin{lstlisting}
+-suc : ∀ m n → m + suc n ≡ suc (m + n)
+-suc zero    n = refl
+-suc (suc m) n = cong suc (+-suc m n)
\end{lstlisting}

\paragraph{Commutative property of addition}

Similarly, by induction on the first argument, we get two sub-goals:

\begin{lstlisting}
+-comm : ∀ m n → m + n ≡ n + m
+-comm zero    n = ?0
+-comm (suc m) n = ?1

?0 : n           ≡ m + zero
?1 : suc (m + n) ≡ m + suc n
\end{lstlisting}

{\lstinline|?0|} can be solved with {\lstinline|+-right-identity|} with a "twist".
The symmetry of equality {\lstinline|sym|} enable us to swap both sides of an equation.

\begin{lstlisting}
+-comm zero    n = sym (+-right-identity n)
\end{lstlisting}

However, it is not obvious how to solve {\lstinline|?1|} straight out.
The proof has to be break into two steps:

\begin{enumerate}
    \item Apply {\lstinline|+-suc|} with {\lstinline|sym|} to the right-hand side
    of the equation to get {\lstinline|suc (m + n) ≡ suc (n + m)|}.
    \item Apply the induction hypothesis to {\lstinline|cong suc|}.
\end{enumerate}

These small pieces of proofs are glued back together with the transitivity of
equality {\lstinline|trans|}.

\begin{lstlisting}
+-comm (suc m) n = trans (cong suc (+-comm m n)) (sym (+-suc n m))
\end{lstlisting}

\paragraph{Equational Reasoning}

We see that proofs are composable just like programs.
But look at the line we have just proven above:

\begin{lstlisting}
trans (cong suc (+-comm m n)) (sym (+-suc n m))
\end{lstlisting}

It is difficult to see what is going on in between these clauses, and it could
get only worse as propositions get more complicated.
Imagine having dozens of {\lstinline|trans|}, {\lstinline|sym|} and {\lstinline|cong|}
spreading everywhere.

Fortunately, these complex proofs can be written in a concise and modular manner
with a simple yet powerful technique called \textit{equational reasoning}.
Agda's flexible mixfix syntax allows the technique to be implemented with just
a few combinators\cite{erikhesselinkpaulvisschers2008}.

This is best illustrated by an example:

\begin{lstlisting}
+-comm : ∀ m n → m + n ≡ n + m
+-comm zero    n = sym (+-right-identity n)
+-comm (suc m) n =
    begin
        suc m + n
    ≡⟨ refl ⟩
        suc (m + n)
    ≡⟨ cong suc (+-comm m n) ⟩
        suc (n + m)
    ≡⟨ sym (+-suc n m) ⟩
        n + suc m
    ∎
\end{lstlisting}

With equational reasoning, we can see how a proposition equates with another,
step by step, justified with theorems. The first and the last step corresponds to
two sides of the equation of the proposition. {\lstinline|begin_|} marks the beginning
of a reasoning; {\lstinline|_≡⟨_⟩_|} chains two propositions with the justication
placed in between; {\lstinline|_∎|} marks the end of a reasoning (\textit{QED}).

These combinators can be further generalized to support preorder equational
reasoning. (Preorders are {\lstinline|reflexive|} and {\lstinline|transitive|})

Suppose we already have {\lstinline|m≤m+n : ∀ m n → m ≤ m + n|} and want to
prove a slightly different theorem.

\begin{lstlisting}
m≤n+m : ∀ m n → m ≤ n + m
m≤n+m m n =
    start
        m
    ≤⟨ m≤m+n m n ⟩
        m + n
    ≈⟨ +-comm m n ⟩
        n + m
    □
\end{lstlisting}

Where {\lstinline|_≤⟨_⟩_|} and {\lstinline|_≈⟨_⟩_|} are respectively transitive
and reflexive combinators.\footnote{Combinators are renamed to prevent conflictions.}


% The set of combinators demonstrated above

\section{Num : a representation for positional numeral systems}\label{representation}
% [draft]

In this section, we will demonstrate how to construct the representation for positional
numeral systems in Agda. The representation is constructed as a datatype, indexed
by the generalizations introduced in section~\ref{introduction}.

\begin{itemize}
    \item \textbf{base}: the base of a numeral system, denoted {\lstinline|b|}.
    \item \textbf{\#digit}: the number of digits, denoted {\lstinline|d|}.
    \item \textbf{offset}: the number where the digits starts from, denoted {\lstinline|o|}.
\end{itemize}

\subsection{Digits}
% [draft]

A system can only have \textbf{finitely many} digits.
Operations on these digits, such as addition, must be \textbf{constant time}.
Notice that the problem size of time complexity we are discussing here refers
only to the value of a numeral. And since the value of digits is independent of
the value of a numeral, time complexity of functions on digits should be trivially
constant.

\subsubsection{Fin}

To represent a digit, we use a datatype that is conventionally called \textit{Fin}
which can be indexed to have some exact number of inhabitants.


\begin{lstlisting}
data Fin : ℕ → Set where
    zero : {n : ℕ} → Fin (suc n)
    suc  : {n : ℕ} (i : Fin n) → Fin (suc n)
\end{lstlisting}

The definition of {\lstinline|Fin|} looks the same as {\lstinline|ℕ|} on the term
level, but different on the type level. The index of a {\lstinline|Fin|} increases
with every {\lstinline|suc|}, and there can only be at most {\lstinline|n|} of
them before reaching {\lstinline|Fin (suc n)|}. In other words, {\lstinline|Fin n|}
has exactly \textit{n} inhabitants.

\textit{Fin} is available in the stardard library, along with other auxiliary
functions:

\begin{itemize}
    \item {\lstinline|toℕ : ∀ {n} → Fin n → ℕ|}
        \\ converts from {\lstinline|Fin n|} to {\lstinline|ℕ|}.
    \item {\lstinline|fromℕ≤ : ∀ {m n} → m < n → Fin n|}
        \\ converts from {\lstinline|ℕ|} to {\lstinline|Fin n|} given the number is small enough.
    \item {\lstinline|#_ : ∀ m {n} {m<n : True (suc m N≤? n)} → Fin n|}
        \\ similar to {\lstinline|fromℕ≤|}, but more convenient, since the proof of {\lstinline|m<n|} is decidable thus can be inferred and made implicit.
    \item {\lstinline|inject≤ : ∀ {m n} → Fin m → m ≤ n → Fin n|}
        \\ converts a smaller {\lstinline|Fin|} to a larger {\lstinline|Fin|}.
\end{itemize}

\subsubsection{Definition}

{\lstinline|Digit|} is simply just a synonym for {\lstinline|Fin|}.

\begin{lstlisting}
Digit : ℕ → Set
Digit = Fin
\end{lstlisting}

Binary digits for example can be represented as:

\begin{lstlisting}
Binary : Set
Binary = Digit 2

零 : Binary
零 = zero

一 : Binary
一 = suc zero
\end{lstlisting}

\subsubsection{Converting from and to natural numbers}

Digit are evaluated together with the offset {\lstinline|o|} of a system.

\begin{lstlisting}
Digit-toℕ : ∀ {d} → Digit d → ℕ → ℕ
Digit-toℕ x o = toℕ x + o
\end{lstlisting}

Not all natural numbers can be converted to digits. The value has to be in certain
range, between $ o $ and $ d + o $.

\begin{lstlisting}
Digit-fromℕ : ∀ {d} → (n o : ℕ) → d + o ≥ n → Digit (suc d)
Digit-fromℕ {d} n o upper-bound with n ∸ o ≤? d
Digit-fromℕ {d} n o upper-bound | yes p = fromℕ≤ (s≤s p)
Digit-fromℕ {d} n o upper-bound | no ¬p = contradiction p ¬p
    where   p : n ∸ o ≤ d
            p = start
                    n ∸ o
                ≤⟨ ∸n-mono o upper-bound ⟩
                    (d + o) ∸ o
                ≤⟨ reflexive (m+n∸n≡m d o) ⟩
                    d
                □
\end{lstlisting}

\subsubsection{Properties}

% A digit is bounded to have
%
% \begin{lstlisting}
% Digit-upper-bound : ∀ {d} → (o : ℕ) → (x : Digit d) → Digit-toℕ x o < d + o
% Digit-upper-bound {d} o x = +n-mono o (bounded x)
%
% Digit-lower-bound : ∀ {d} → (o : ℕ) → (x : Digit d) → Digit-toℕ x o ≥ o
% Digit-lower-bound {d} o x = m≤n+m o (toℕ x)
% \end{lstlisting}

\subsection{Num}
% [draft]

Numerals in positional numeral systems are composed of sequences of \textbf{digits}.

\subsubsection{Definition}
The definition of {\lstinline|Numeral|} is similar to that of {\lstinline|List|},
except that a {\lstinline|Numeral|} must contain at least one digit while a list
may contain no elements at all. The most significant digit is placed in {\lstinline|_∙|}
while the least significant digit is placed at the end of the sequence.
{\lstinline|Numeral|} is indexed by all three generalizations.

\begin{lstlisting}
infixr 5 _∷_

data Numeral : ℕ → ℕ → ℕ → Set where
    _∙  : ∀ {b d o} → Digit d → Numeral b d o
    _∷_ : ∀ {b d o} → Digit d → Numeral b d o → Numeral b d o
\end{lstlisting}

The decimal number "2016" for example can be represented as:

\begin{lstlisting}
MMXVI : Numeral 10 10 0
MMXVI = # 6 ∷ # 1 ∷ # 0 ∷ (# 2) ∙
\end{lstlisting}

\subsubsection{Converting to natural numbers}

Converting to natural numbers is fairly trivial:

\begin{lstlisting}
⟦_⟧ : ∀ {b d o} → (xs : Numeral b d o) → ℕ
⟦_⟧ {_} {_} {o} (x ∙)    = Digit-toℕ x o
⟦_⟧ {b} {_} {o} (x ∷ xs) = Digit-toℕ x o + ⟦ xs ⟧ * b
\end{lstlisting}

\section{Dissecting Num: Properties of different kinds of numeral systems}\label{views}

There are many kinds of numeral systems inhabit in {\lstinline|Num|}.
Some have infinitely many numerals and some have none.

We sort the systems in {\lstinline|Num|} into four groups, each of them have
different interesting properties.

\subsection{Views}


\begin{lstlisting}
data NumView : ℕ → ℕ → ℕ → Set where
    NullBase    : ∀   d o                            → NumView 0       (suc d) o
    NoDigits    : ∀ b o                              → NumView b       0       o
    AllZeros    : ∀ b                                → NumView (suc b) 1       0
    Proper      : ∀ b d o → (proper : suc d + o ≥ 2) → NumView (suc b) (suc d) o
\begin{lstlisting}

\subsection{Maximum}

A number is said to be \textit{maximum} if there are no other number greater than
itself.

\begin{lstlisting}
Maximum : ∀ {b d o} → (xs : Numeral b d o) → Set
Maximum {b} {d} {o} xs = ∀ (ys : Numeral b d o) → ⟦ xs ⟧ ≥ ⟦ ys ⟧
\end{lstlisting}


\section{Conclusions}\label{conclusions}

\bibliographystyle{abbrv}
\bibliography{thesis}
\end{document}
